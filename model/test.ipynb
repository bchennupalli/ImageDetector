{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f85eba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original training CSV...\n",
      "Total training samples: 79950\n",
      "Randomly selecting 5000 images from the training data...\n",
      "Random sample saved to /Users/bhaskarpramodchennupalli/Documents/Image_Detector/random_5000_train.csv\n",
      "\n",
      "Defining image transformations...\n",
      "Transformations defined.\n",
      "\n",
      "Creating dataset from the random sample CSV...\n",
      "[Dataset] Loaded 5000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/random_5000_train.csv\n",
      "Dataset created with 5000 images.\n",
      "Creating DataLoader with batch size 32...\n",
      "DataLoader created.\n",
      "\n",
      "Defining the model architecture...\n",
      "Instantiating the model...\n",
      "Loading model weights from: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/model.pth\n",
      "Model loaded and set to evaluation mode.\n",
      "\n",
      "Running evaluation on the random 5000 training images...\n",
      "\n",
      "Test Results on 5000 Random Training Images:\n",
      "Accuracy: 99.46%\n",
      "Correct: 4973/5000\n",
      "\n",
      "Saving predictions to CSV...\n",
      "Predictions saved to '/Users/bhaskarpramodchennupalli/Documents/Image_Detector/random_5000_train_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "base_dir = '/Users/bhaskarpramodchennupalli/Documents/Image_Detector'\n",
    "csv_path = os.path.join(base_dir, 'train.csv')\n",
    "train_data_dir = os.path.join(base_dir, 'train_data')\n",
    "model_path = os.path.join(base_dir, 'model.pth')\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Read the Original CSV and Randomly Select 5000 Images\n",
    "# ----------------------------\n",
    "print(\"Loading original training CSV...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "print(f\"Total training samples: {len(df)}\")\n",
    "\n",
    "print(\"Randomly selecting 5000 images from the training data...\")\n",
    "df_sample = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "sample_csv_path = os.path.join(base_dir, 'random_5000_train.csv')\n",
    "df_sample.to_csv(sample_csv_path, index=False)\n",
    "print(f\"Random sample saved to {sample_csv_path}\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Define the Custom Dataset Class\n",
    "# ----------------------------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        print(f\"[Dataset] Loaded {len(self.data_frame)} records from {csv_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image path and label from the CSV\n",
    "        file_path = self.data_frame.iloc[idx]['file_name']\n",
    "        filename = os.path.basename(file_path)\n",
    "        full_path = os.path.join(train_data_dir, filename)\n",
    "        # Open the image and convert to RGB\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.data_frame.iloc[idx]['label']\n",
    "        return image, label\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Define Transformations and Create DataLoader\n",
    "# ----------------------------\n",
    "print(\"Defining image transformations...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # Uncomment the next line if normalization was used during training:\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "print(\"Transformations defined.\\n\")\n",
    "\n",
    "print(\"Creating dataset from the random sample CSV...\")\n",
    "dataset = CustomImageDataset(csv_file=sample_csv_path, transform=transform)\n",
    "print(f\"Dataset created with {len(dataset)} images.\")\n",
    "\n",
    "batch_size = 32\n",
    "print(f\"Creating DataLoader with batch size {batch_size}...\")\n",
    "# Use num_workers=0 if running in Jupyter; otherwise, adjust as needed\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(\"DataLoader created.\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Define the Model Architecture and Load the Trained Model\n",
    "# ----------------------------\n",
    "print(\"Defining the model architecture...\")\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # For an input image of size 224x224, one pooling reduces the spatial dimensions to 112x112.\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(16 * 112 * 112, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "print(\"Instantiating the model...\")\n",
    "model = SimpleCNN()\n",
    "print(\"Loading model weights from:\", model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "print(\"Model loaded and set to evaluation mode.\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Evaluate the Model on the Random Sample\n",
    "# ----------------------------\n",
    "print(\"Running evaluation on the random 5000 training images...\")\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        outputs = model(images)\n",
    "        # Convert logits to probabilities and then to binary predictions (0 or 1)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        total += labels.size(0)\n",
    "        correct += (preds.squeeze() == labels.float()).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(\"\\nTest Results on 5000 Random Training Images:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Correct: {correct}/{total}\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 6: Optional â€“ Save Predictions to CSV\n",
    "# ----------------------------\n",
    "print(\"Saving predictions to CSV...\")\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for images, _ in dataloader:\n",
    "        outputs = model(images)\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        all_preds.extend(preds.cpu().numpy().flatten().tolist())\n",
    "\n",
    "df_sample['prediction'] = all_preds\n",
    "pred_csv_path = os.path.join(base_dir, 'random_5000_train_predictions.csv')\n",
    "df_sample.to_csv(pred_csv_path, index=False)\n",
    "print(f\"Predictions saved to '{pred_csv_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffd5fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original training CSV...\n",
      "Total training samples: 79950\n",
      "\n",
      "Defining image transformations...\n",
      "Transformations defined.\n",
      "\n",
      "Defining the model architecture...\n",
      "Instantiating the model...\n",
      "Loading model weights from: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/model.pth\n",
      "Model loaded and set to evaluation mode.\n",
      "\n",
      "=== Evaluating Sample Size: 50 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_50.csv\n",
      "[Dataset] Loaded 50 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_50.csv\n",
      "Test Results on 50 Random Training Images:\n",
      "Accuracy: 98.00%\n",
      "Correct: 49/50\n",
      "\n",
      "=== Evaluating Sample Size: 100 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_100.csv\n",
      "[Dataset] Loaded 100 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_100.csv\n",
      "Test Results on 100 Random Training Images:\n",
      "Accuracy: 99.00%\n",
      "Correct: 99/100\n",
      "\n",
      "=== Evaluating Sample Size: 500 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_500.csv\n",
      "[Dataset] Loaded 500 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_500.csv\n",
      "Test Results on 500 Random Training Images:\n",
      "Accuracy: 99.00%\n",
      "Correct: 495/500\n",
      "\n",
      "=== Evaluating Sample Size: 1000 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_1000.csv\n",
      "[Dataset] Loaded 1000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_1000.csv\n",
      "Test Results on 1000 Random Training Images:\n",
      "Accuracy: 99.40%\n",
      "Correct: 994/1000\n",
      "\n",
      "=== Evaluating Sample Size: 5000 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_5000.csv\n",
      "[Dataset] Loaded 5000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_5000.csv\n",
      "Test Results on 5000 Random Training Images:\n",
      "Accuracy: 99.46%\n",
      "Correct: 4973/5000\n",
      "\n",
      "=== Evaluating Sample Size: 10000 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_10000.csv\n",
      "[Dataset] Loaded 10000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_10000.csv\n",
      "Test Results on 10000 Random Training Images:\n",
      "Accuracy: 99.43%\n",
      "Correct: 9943/10000\n",
      "\n",
      "=== Evaluating Sample Size: 15000 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_15000.csv\n",
      "[Dataset] Loaded 15000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_15000.csv\n",
      "Test Results on 15000 Random Training Images:\n",
      "Accuracy: 99.37%\n",
      "Correct: 14905/15000\n",
      "\n",
      "=== Evaluating Sample Size: 25000 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_25000.csv\n",
      "[Dataset] Loaded 25000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_25000.csv\n",
      "Test Results on 25000 Random Training Images:\n",
      "Accuracy: 99.30%\n",
      "Correct: 24824/25000\n",
      "\n",
      "=== Evaluating Sample Size: 40000 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_40000.csv\n",
      "[Dataset] Loaded 40000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_40000.csv\n",
      "Test Results on 40000 Random Training Images:\n",
      "Accuracy: 99.26%\n",
      "Correct: 39705/40000\n",
      "\n",
      "=== Evaluating Sample Size: 60000 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_60000.csv\n",
      "[Dataset] Loaded 60000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_60000.csv\n",
      "Test Results on 60000 Random Training Images:\n",
      "Accuracy: 99.27%\n",
      "Correct: 59564/60000\n",
      "\n",
      "=== Evaluating Sample Size: 75000 ===\n",
      "Temporary sample CSV saved to: /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_75000.csv\n",
      "[Dataset] Loaded 75000 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_75000.csv\n",
      "Test Results on 75000 Random Training Images:\n",
      "Accuracy: 99.28%\n",
      "Correct: 74457/75000\n",
      "\n",
      "Deleting temporary CSV files...\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_50.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_100.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_500.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_1000.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_5000.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_10000.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_15000.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_25000.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_40000.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_60000.csv\n",
      "Deleted /Users/bhaskarpramodchennupalli/Documents/Image_Detector/temp_sample_75000.csv\n",
      "All temporary files deleted.\n",
      "\n",
      "=== Evaluation Process Complete ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "base_dir = '/Users/bhaskarpramodchennupalli/Documents/Image_Detector'\n",
    "csv_path = os.path.join(base_dir, 'train.csv')\n",
    "train_data_dir = os.path.join(base_dir, 'train_data')\n",
    "model_path = os.path.join(base_dir, 'model.pth')\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Read the Original CSV\n",
    "# ----------------------------\n",
    "print(\"Loading original training CSV...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "if 'Unnamed: 0' in df.columns:\n",
    "    df = df.drop(columns=['Unnamed: 0'])\n",
    "total_samples = len(df)\n",
    "print(f\"Total training samples: {total_samples}\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Define the Custom Dataset Class\n",
    "# ----------------------------\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        print(f\"[Dataset] Loaded {len(self.data_frame)} records from {csv_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image path and label from the CSV\n",
    "        file_path = self.data_frame.iloc[idx]['file_name']\n",
    "        filename = os.path.basename(file_path)\n",
    "        full_path = os.path.join(train_data_dir, filename)\n",
    "        # Open the image and convert to RGB\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.data_frame.iloc[idx]['label']\n",
    "        return image, label\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Define Transformations\n",
    "# ----------------------------\n",
    "print(\"Defining image transformations...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # Uncomment the following if you used normalization during training:\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "print(\"Transformations defined.\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Define the Model Architecture and Load the Trained Model\n",
    "# ----------------------------\n",
    "print(\"Defining the model architecture...\")\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # For an image of size 224x224, one pooling reduces dimensions to 112x112.\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(16 * 112 * 112, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "print(\"Instantiating the model...\")\n",
    "model = SimpleCNN()\n",
    "print(\"Loading model weights from:\", model_path)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "print(\"Model loaded and set to evaluation mode.\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Evaluate on Multiple Sample Sizes\n",
    "# ----------------------------\n",
    "# Define the sample sizes you wish to evaluate\n",
    "sample_sizes = [50, 100, 500, 1000, 5000, 10000, 15000, 25000, 40000, 60000, 75000]\n",
    "\n",
    "# List to keep track of temporary CSV files so we can delete them later\n",
    "temp_csv_files = []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    print(f\"=== Evaluating Sample Size: {sample_size} ===\")\n",
    "    if sample_size > total_samples:\n",
    "        print(f\"Requested sample size {sample_size} is greater than total available samples {total_samples}. Skipping...\\n\")\n",
    "        continue\n",
    "\n",
    "    # Randomly sample the requested number of images from the full CSV\n",
    "    df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "    temp_csv = os.path.join(base_dir, f\"temp_sample_{sample_size}.csv\")\n",
    "    df_sample.to_csv(temp_csv, index=False)\n",
    "    temp_csv_files.append(temp_csv)\n",
    "    print(f\"Temporary sample CSV saved to: {temp_csv}\")\n",
    "\n",
    "    # Create dataset and DataLoader for the sample\n",
    "    dataset = CustomImageDataset(csv_file=temp_csv, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)  # Set num_workers=0 for Jupyter\n",
    "\n",
    "    # Evaluate model on this sample\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            preds = torch.round(torch.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (preds.squeeze() == labels.float()).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Results on {sample_size} Random Training Images:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Correct: {correct}/{total}\\n\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 6: Clean Up Temporary Files\n",
    "# ----------------------------\n",
    "print(\"Deleting temporary CSV files...\")\n",
    "for temp_file in temp_csv_files:\n",
    "    if os.path.exists(temp_file):\n",
    "        os.remove(temp_file)\n",
    "        print(f\"Deleted {temp_file}\")\n",
    "print(\"All temporary files deleted.\\n\")\n",
    "\n",
    "print(\"=== Evaluation Process Complete ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c60942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2af64607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "[TestDataset] Loaded 5540 records from /Users/bhaskarpramodchennupalli/Documents/Image_Detector/test.csv\n",
      "Test dataset size: 5540 images\n",
      "Model loaded and set to evaluation mode.\n",
      "Submission saved to /Users/bhaskarpramodchennupalli/Documents/Image_Detector/submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# ---------------------------\n",
    "# Device Setup\n",
    "# ---------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define Paths\n",
    "# ---------------------------\n",
    "base_dir = '/Users/bhaskarpramodchennupalli/Documents/Image_Detector'\n",
    "test_csv_path = os.path.join(base_dir, 'test.csv')  # CSV containing test image info\n",
    "test_data_dir = os.path.join(base_dir, 'test_data')  # Directory containing test images\n",
    "model_path = os.path.join(base_dir, 'best_model.pth')  # Saved best model\n",
    "\n",
    "# ---------------------------\n",
    "# Define a Custom Dataset for Test Data\n",
    "# ---------------------------\n",
    "class TestImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Expects the CSV file to have a column named 'id' that contains the relative image path.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        print(f\"[TestDataset] Loaded {len(self.df)} records from {csv_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use the 'id' column (since your test CSV contains only this column)\n",
    "        file_name = self.df.iloc[idx]['id']\n",
    "        # Construct full path: if the CSV contains a relative path, you can join it with the test_data_dir\n",
    "        full_path = os.path.join(test_data_dir, os.path.basename(file_name))\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, file_name\n",
    "\n",
    "# ---------------------------\n",
    "# Define Image Transformations\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # If you used normalization during training, uncomment the following:\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Create Test Dataset and DataLoader\n",
    "# ---------------------------\n",
    "test_dataset = TestImageDataset(csv_file=test_csv_path, transform=transform)\n",
    "batch_size = 32\n",
    "# Use num_workers=0 in Jupyter Notebook to avoid multiprocessing issues\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "print(f\"Test dataset size: {len(test_dataset)} images\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define the Model Architecture\n",
    "# ---------------------------\n",
    "# Here, we assume your best model was ResNet18 fine-tuned for binary classification.\n",
    "def create_resnet18():\n",
    "    weights = models.ResNet18_Weights.DEFAULT\n",
    "    model = models.resnet18(weights=weights)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 1)\n",
    "    return model\n",
    "\n",
    "model = create_resnet18()\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded and set to evaluation mode.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Run Inference\n",
    "# ---------------------------\n",
    "all_preds = []\n",
    "all_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, file_names in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        # Convert logits to probabilities using sigmoid, then threshold to obtain binary predictions.\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).int()\n",
    "        all_preds.extend(preds.cpu().numpy().flatten().tolist())\n",
    "        all_ids.extend(file_names)\n",
    "\n",
    "# ---------------------------\n",
    "# Save Predictions to CSV\n",
    "# ---------------------------\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': all_ids,\n",
    "    'label': all_preds\n",
    "})\n",
    "submission_csv = os.path.join(base_dir, 'submission.csv')\n",
    "submission_df.to_csv(submission_csv, index=False)\n",
    "print(f\"Submission saved to {submission_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6aa835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Each File Seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc14b474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model loaded and set to evaluation mode.\n",
      "\n",
      "Prediction for '/Users/bhaskarpramodchennupalli/Documents/Image_Detector/test_data/0a14a73f896a46009d545bd2b9960294.jpg': 0 (Probability: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Loads a ResNet18 model fine-tuned for binary classification.\n",
    "    \"\"\"\n",
    "    # Use the recommended weights argument for torchvision>=0.13.\n",
    "    weights = models.ResNet18_Weights.DEFAULT\n",
    "    model = models.resnet18(weights=weights)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    # Replace the final fully-connected layer for binary classification.\n",
    "    model.fc = nn.Linear(num_ftrs, 1)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def predict_image(model, image_path, device):\n",
    "    \"\"\"\n",
    "    Loads an image, applies transformations, and returns the binary prediction\n",
    "    along with the probability.\n",
    "    \"\"\"\n",
    "    # Define the transformation (adjust normalization if used during training)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # Uncomment the next line if normalization was used during training:\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Apply transformation and add batch dimension\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        # Apply sigmoid to get probability (assuming BCEWithLogitsLoss was used during training)\n",
    "        prob = torch.sigmoid(output)\n",
    "        # Get binary prediction: 1 if probability > 0.5 else 0\n",
    "        pred = (prob > 0.5).int().item()\n",
    "    \n",
    "    return pred, prob.item()\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "# Set up the device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hardcode the image path\n",
    "image_path = \"/Users/bhaskarpramodchennupalli/Documents/Image_Detector/test_data/0a14a73f896a46009d545bd2b9960294.jpg\"\n",
    "\n",
    "# Path to your saved model (adjust if necessary)\n",
    "model_path = \"/Users/bhaskarpramodchennupalli/Documents/Image_Detector/best_model.pth\"\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path, device)\n",
    "print(\"Model loaded and set to evaluation mode.\\n\")\n",
    "\n",
    "# Predict on the hardcoded image\n",
    "prediction, probability = predict_image(model, image_path, device)\n",
    "if prediction is not None:\n",
    "    print(f\"Prediction for '{image_path}': {prediction} (Probability: {probability:.4f})\")\n",
    "else:\n",
    "    print(\"Prediction failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d47e50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
